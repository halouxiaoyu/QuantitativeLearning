#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾é€‰æ‹©æ¨¡å—
å¸®åŠ©è¯†åˆ«å’Œé€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼Œå‡å°‘å™ªéŸ³ï¼Œæé«˜æ¨¡å‹æ€§èƒ½
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

class FeatureSelector:
    """ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self, features_dir='features', results_dir='results'):
        self.features_dir = features_dir
        self.results_dir = results_dir
        self.ensure_directories()
        
    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(os.path.join(self.results_dir, 'feature_analysis'), exist_ok=True)
    
    def load_features(self, stock_code):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        stock_dir = os.path.join(self.features_dir, stock_code)
        if not os.path.exists(stock_dir):
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ•°æ®: {stock_code}")
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾æ–‡ä»¶
        import glob
        pattern = f"{stock_code}_features_*.csv"
        files = glob.glob(os.path.join(stock_dir, pattern))
        
        if not files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {stock_code}")
            return None
        
        # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getctime)
        print(f"ğŸ“‚ åŠ è½½ç‰¹å¾: {os.path.basename(latest_file)}")
        
        try:
            feat = pd.read_csv(latest_file, index_col=0, parse_dates=True)
            print(f"âœ… ç‰¹å¾åŠ è½½æˆåŠŸ: {len(feat)} æ¡è®°å½•")
            return feat
        except Exception as e:
            print(f"âŒ ç‰¹å¾åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_data(self, feat, start_date='2021-01-01', end_date='2024-12-31'):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print(f"ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®: {start_date} åˆ° {end_date}")
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        training_data = feat[(feat.index >= start_dt) & (feat.index <= end_dt)].copy()
        
        if len(training_data) == 0:
            print(f"âŒ åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
            return None, None, None
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in training_data.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
        
        if len(feature_cols) == 0:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            return None, None, None
        
        X = training_data[feature_cols].values
        y = (training_data['close'].shift(-1) > training_data['close']).astype(int)
        
        # ç§»é™¤æœ€åä¸€è¡Œï¼ˆå› ä¸ºæ²¡æœ‰ä¸‹ä¸€å¤©çš„ä»·æ ¼ï¼‰
        X = X[:-1]
        y = y[:-1]
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
        X = X[valid_mask]
        y = y[valid_mask]
        
        print(f"ğŸ” æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(X)} æ¡")
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X.shape}")
        print(f"ğŸ¯ æ ‡ç­¾åˆ†å¸ƒ: ä¸Šæ¶¨ {np.sum(y)} ({np.sum(y)/len(y):.1%}), ä¸‹è·Œ {np.sum(~y)} ({np.sum(~y)/len(y):.1%})")
        
        return X, y, feature_cols
    
    def analyze_feature_importance(self, stock_code, start_date='2021-01-01', end_date='2024-12-31', n_features=20):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print(f"ğŸ” åˆ†æç‰¹å¾é‡è¦æ€§: {stock_code}")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        feat = self.load_features(stock_code)
        if feat is None:
            return None
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y, feature_cols = self.prepare_data(feat, start_date, end_date)
        if X is None:
            return None
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # 1. éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
        print("ğŸŒ² ä½¿ç”¨éšæœºæ£®æ—åˆ†æç‰¹å¾é‡è¦æ€§...")
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf.fit(X_scaled, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = rf.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print(f"ğŸ“Š éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§æ’åº (å‰10å):")
        for i, row in feature_importance_df.head(10).iterrows():
            print(f"   {row['feature']:25} : {row['importance']:.4f}")
        
        # 2. ç»Ÿè®¡æ£€éªŒç‰¹å¾é€‰æ‹©
        print("\nğŸ“ˆ ä½¿ç”¨ç»Ÿè®¡æ£€éªŒé€‰æ‹©ç‰¹å¾...")
        selector_f = SelectKBest(score_func=f_classif, k=n_features)
        X_selected_f = selector_f.fit_transform(X_scaled, y)
        selected_features_f = [feature_cols[i] for i in selector_f.get_support(indices=True)]
        
        print(f"ğŸ“Š Fæ£€éªŒé€‰æ‹©çš„ç‰¹å¾ ({len(selected_features_f)} ä¸ª):")
        for i, feature in enumerate(selected_features_f, 1):
            print(f"   {i:2d}. {feature}")
        
        # 3. äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©
        print("\nğŸ“Š ä½¿ç”¨äº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾...")
        selector_mi = SelectKBest(score_func=mutual_info_classif, k=n_features)
        X_selected_mi = selector_mi.fit_transform(X_scaled, y)
        selected_features_mi = [feature_cols[i] for i in selector_mi.get_support(indices=True)]
        
        print(f"ğŸ“Š äº’ä¿¡æ¯é€‰æ‹©çš„ç‰¹å¾ ({len(selected_features_mi)} ä¸ª):")
        for i, feature in enumerate(selected_features_mi, 1):
            print(f"   {i:2d}. {feature}")
        
        # 4. äº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”
        print("\nğŸ§ª äº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”...")
        
        # åŸå§‹ç‰¹å¾
        cv_scores_original = cross_val_score(rf, X_scaled, y, cv=5, scoring='accuracy')
        print(f"ğŸ“Š åŸå§‹ç‰¹å¾ (42ä¸ª): {cv_scores_original.mean():.3f} Â± {cv_scores_original.std() * 2:.3f}")
        
        # éšæœºæ£®æ—é€‰æ‹©çš„ç‰¹å¾
        top_features_rf = feature_importance_df.head(n_features)['feature'].tolist()
        X_rf_selected = X[:, [feature_cols.index(f) for f in top_features_rf]]
        X_rf_scaled = scaler.fit_transform(X_rf_selected)
        cv_scores_rf = cross_val_score(rf, X_rf_scaled, y, cv=5, scoring='accuracy')
        print(f"ğŸ“Š éšæœºæ£®æ—é€‰æ‹© ({n_features}ä¸ª): {cv_scores_rf.mean():.3f} Â± {cv_scores_rf.std() * 2:.3f}")
        
        # Fæ£€éªŒé€‰æ‹©çš„ç‰¹å¾
        X_f_scaled = scaler.fit_transform(X_selected_f)
        cv_scores_f = cross_val_score(rf, X_f_scaled, y, cv=5, scoring='accuracy')
        print(f"ğŸ“Š Fæ£€éªŒé€‰æ‹© ({n_features}ä¸ª): {cv_scores_f.mean():.3f} Â± {cv_scores_f.std() * 2:.3f}")
        
        # 5. ç”Ÿæˆç‰¹å¾åˆ†ææŠ¥å‘Š
        analysis_result = {
            'stock_code': stock_code,
            'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'original_features': len(feature_cols),
            'selected_features': n_features,
            'feature_importance': feature_importance_df.to_dict('records'),
            'rf_selected_features': top_features_rf,
            'f_test_selected_features': selected_features_f,
            'mi_selected_features': selected_features_mi,
            'cv_performance': {
                'original': {
                    'mean': float(cv_scores_original.mean()),
                    'std': float(cv_scores_original.std()),
                    'scores': cv_scores_original.tolist()
                },
                'rf_selected': {
                    'mean': float(cv_scores_rf.mean()),
                    'std': float(cv_scores_rf.std()),
                    'scores': cv_scores_rf.tolist()
                },
                'f_test_selected': {
                    'mean': float(cv_scores_f.mean()),
                    'std': float(cv_scores_f.std()),
                    'scores': cv_scores_f.tolist()
                }
            }
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        self.save_analysis_result(analysis_result)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_feature_plots(feature_importance_df, analysis_result)
        
        return analysis_result
    
    def save_analysis_result(self, analysis_result):
        """ä¿å­˜ç‰¹å¾åˆ†æç»“æœ"""
        try:
            stock_code = analysis_result['stock_code']
            timestamp = analysis_result['analysis_date']
            
            # åˆ›å»ºè‚¡ç¥¨å­ç›®å½•
            stock_dir = os.path.join(self.results_dir, 'feature_analysis', stock_code)
            os.makedirs(stock_dir, exist_ok=True)
            
            # ä¿å­˜åˆ†æç»“æœ
            result_file = os.path.join(stock_dir, f"{stock_code}_feature_analysis_{timestamp}.json")
            import json
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ç‰¹å¾åˆ†æç»“æœå·²ä¿å­˜: {os.path.basename(result_file)}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç‰¹å¾åˆ†æç»“æœå¤±è´¥: {e}")
    
    def generate_feature_plots(self, feature_importance_df, analysis_result):
        """ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨"""
        try:
            stock_code = analysis_result['stock_code']
            timestamp = analysis_result['analysis_date']
            
            # åˆ›å»ºå›¾è¡¨ç›®å½•
            plots_dir = os.path.join(self.results_dir, 'feature_analysis', stock_code, 'plots')
            os.makedirs(plots_dir, exist_ok=True)
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # 1. ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
            plt.figure(figsize=(12, 8))
            top_features = feature_importance_df.head(20)
            plt.barh(range(len(top_features)), top_features['importance'])
            plt.yticks(range(len(top_features)), top_features['feature'])
            plt.xlabel('ç‰¹å¾é‡è¦æ€§')
            plt.title(f'{stock_code} - ç‰¹å¾é‡è¦æ€§æ’åº (å‰20å)')
            plt.tight_layout()
            
            plot_file = os.path.join(plots_dir, f"{stock_code}_feature_importance_{timestamp}.png")
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜: {os.path.basename(plot_file)}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç‰¹å¾å›¾è¡¨å¤±è´¥: {e}")
    
    def get_optimal_feature_set(self, stock_code, start_date='2021-01-01', end_date='2024-12-31'):
        """è·å–æœ€ä¼˜ç‰¹å¾é›†"""
        print(f"ğŸ¯ å¯»æ‰¾æœ€ä¼˜ç‰¹å¾é›†: {stock_code}")
        print("=" * 60)
        
        # åˆ†æç‰¹å¾é‡è¦æ€§
        analysis_result = self.analyze_feature_importance(stock_code, start_date, end_date)
        if analysis_result is None:
            return None
        
        # æ ¹æ®äº¤å‰éªŒè¯æ€§èƒ½é€‰æ‹©æœ€ä½³ç‰¹å¾é›†
        cv_performance = analysis_result['cv_performance']
        
        best_method = None
        best_score = 0
        
        for method, scores in cv_performance.items():
            if scores['mean'] > best_score:
                best_score = scores['mean']
                best_method = method
        
        print(f"\nğŸ† æœ€ä½³ç‰¹å¾é€‰æ‹©æ–¹æ³•: {best_method}")
        print(f"ğŸ“Š æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®ç‡: {best_score:.3f}")
        
        if best_method == 'rf_selected':
            optimal_features = analysis_result['rf_selected_features']
        elif best_method == 'f_test_selected':
            optimal_features = analysis_result['f_test_selected_features']
        else:
            optimal_features = analysis_result['rf_selected_features']  # é»˜è®¤ä½¿ç”¨éšæœºæ£®æ—é€‰æ‹©
        
        print(f"ğŸ¯ æ¨èç‰¹å¾æ•°é‡: {len(optimal_features)}")
        print(f"ğŸ“‹ æ¨èç‰¹å¾åˆ—è¡¨:")
        for i, feature in enumerate(optimal_features, 1):
            print(f"   {i:2d}. {feature}")
        
        return optimal_features

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç‰¹å¾é€‰æ‹©æ¨¡å—")
    print("=" * 60)
    
    fs = FeatureSelector()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾æ•°æ®
    if not os.path.exists(fs.features_dir):
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾ç›®å½•!")
        print("   è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹æ¨¡å—æ„å»ºç‰¹å¾")
        return
    
    # è·å–å¯ç”¨çš„è‚¡ç¥¨
    stock_dirs = [d for d in os.listdir(fs.features_dir) if os.path.isdir(os.path.join(fs.features_dir, d))]
    
    if not stock_dirs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ•°æ®!")
        print("   è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹æ¨¡å—æ„å»ºç‰¹å¾")
        return
    
    print(f"ğŸ“Š å‘ç° {len(stock_dirs)} åªè‚¡ç¥¨çš„ç‰¹å¾:")
    for i, stock in enumerate(stock_dirs, 1):
        print(f"   {i}. {stock}")
    
    # é€‰æ‹©ç¬¬ä¸€åªè‚¡ç¥¨è¿›è¡Œåˆ†æ
    if stock_dirs:
        test_stock = stock_dirs[0]
        print(f"\nğŸ§ª æµ‹è¯•ç‰¹å¾é€‰æ‹©: {test_stock}")
        
        # è·å–æœ€ä¼˜ç‰¹å¾é›†
        optimal_features = fs.get_optimal_feature_set(test_stock)
        
        if optimal_features:
            print(f"\nâœ… ç‰¹å¾é€‰æ‹©å®Œæˆ!")
            print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨ {len(optimal_features)} ä¸ªæœ€é‡è¦çš„ç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        else:
            print(f"\nâŒ ç‰¹å¾é€‰æ‹©å¤±è´¥")

if __name__ == "__main__":
    main()
