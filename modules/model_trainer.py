#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒä¸ä¿å­˜æ¨¡å—
æ”¯æŒå¤šè‚¡ç¥¨æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œä¿å­˜
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import joblib
import json

# å°è¯•å¯¼å…¥XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoostæœªå®‰è£…ï¼Œå°†ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºå¤‡é€‰")

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, features_dir, models_dir, results_dir):
        self.features_dir = features_dir
        self.models_dir = models_dir
        self.results_dir = results_dir
        
        # é»˜è®¤æ—¶é—´åˆ’åˆ†é…ç½®
        self.default_time_config = {
            'training_start': '2021-01-01',    # è®­ç»ƒå¼€å§‹æ—¶é—´
            'training_end': '2024-12-31',      # è®­ç»ƒç»“æŸæ—¶é—´
            'validation_start': '2025-01-01',  # éªŒè¯å¼€å§‹æ—¶é—´
            'validation_end': '2025-12-31',    # éªŒè¯ç»“æŸæ—¶é—´
            'test_ratio': 0.2                  # æµ‹è¯•é›†æ¯”ä¾‹
        }
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(models_dir, exist_ok=True)
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(os.path.join(results_dir, 'training_reports'), exist_ok=True)
    
    def load_features(self, stock_code):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        stock_dir = os.path.join(self.features_dir, stock_code)
        if not os.path.exists(stock_dir):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰¹å¾ç›®å½•: {stock_code}")
        
        # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾æ–‡ä»¶
        feature_files = [f for f in os.listdir(stock_dir) if f.endswith('.csv') and 'features' in f]
        if not feature_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {stock_code}")
        
        latest_file = sorted(feature_files)[-1]
        file_path = os.path.join(stock_dir, latest_file)
        
        print(f"ğŸ“‚ åŠ è½½ç‰¹å¾: {os.path.basename(latest_file)}")
        
        try:
            feat = pd.read_csv(file_path, index_col=0, parse_dates=True)
            print(f"   âœ… ç‰¹å¾åŠ è½½æˆåŠŸ: {len(feat)} æ¡è®°å½•")
            print(f"   ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {feat.index.min().strftime('%Y-%m-%d')} åˆ° {feat.index.max().strftime('%Y-%m-%d')}")
            return feat
        except Exception as e:
            raise Exception(f"ç‰¹å¾åŠ è½½å¤±è´¥: {e}")
    
    def prepare_training_data(self, stock_code_or_data, start_date=None, end_date=None, test_ratio=None, return_split=True):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´åˆ’åˆ†"""
        print(f"ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®: {stock_code_or_data}")
        print("=" * 50)
        
        # æ£€æŸ¥è¾“å…¥ç±»å‹
        if isinstance(stock_code_or_data, str):
            # å¦‚æœæ˜¯è‚¡ç¥¨ä»£ç ï¼ŒåŠ è½½ç‰¹å¾æ•°æ®
            feat = self.load_features(stock_code_or_data)
        elif hasattr(stock_code_or_data, 'columns'):
            # å¦‚æœæ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨
            feat = stock_code_or_data
        else:
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯è‚¡ç¥¨ä»£ç å­—ç¬¦ä¸²æˆ–DataFrame")
        
        # ä½¿ç”¨é»˜è®¤æ—¶é—´é…ç½®
        if start_date is None:
            start_date = self.default_time_config['training_start']
        if end_date is None:
            end_date = self.default_time_config['training_end']
        if test_ratio is None:
            test_ratio = self.default_time_config['test_ratio']
        
        print(f"ğŸ“… è®­ç»ƒæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"ğŸ“Š æµ‹è¯•é›†æ¯”ä¾‹: {test_ratio:.1%}")
        
        # è¿‡æ»¤è®­ç»ƒæ•°æ®
        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)
        
        # ç¡®ä¿æ•°æ®åœ¨æŒ‡å®šèŒƒå›´å†…
        available_start = feat.index.min()
        available_end = feat.index.max()
        
        if start_date < available_start:
            print(f"âš ï¸  è¯·æ±‚çš„å¼€å§‹æ—¶é—´ {start_date.strftime('%Y-%m-%d')} æ—©äºå¯ç”¨æ•°æ® {available_start.strftime('%Y-%m-%d')}ï¼Œä½¿ç”¨å¯ç”¨æ•°æ®å¼€å§‹æ—¶é—´")
            start_date = available_start
        
        if end_date > available_end:
            print(f"âš ï¸  è¯·æ±‚çš„ç»“æŸæ—¶é—´ {end_date.strftime('%Y-%m-%d')} æ™šäºå¯ç”¨æ•°æ® {available_end.strftime('%Y-%m-%d')}ï¼Œä½¿ç”¨å¯ç”¨æ•°æ®ç»“æŸæ—¶é—´")
            end_date = available_end
        
        # è¿‡æ»¤è®­ç»ƒæ•°æ®
        training_data = feat[(feat.index >= start_date) & (feat.index <= end_date)].copy()
        
        if len(training_data) == 0:
            raise ValueError(f"åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ•°æ®: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
        
        print(f"ğŸ“ˆ è®­ç»ƒæ•°æ®: {len(training_data)} æ¡è®°å½•")
        print(f"ğŸ“… å®é™…è®­ç»ƒèŒƒå›´: {training_data.index.min().strftime('%Y-%m-%d')} åˆ° {training_data.index.max().strftime('%Y-%m-%d')}")
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in training_data.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
        
        if len(feature_cols) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
        
        X = training_data[feature_cols].values
        
        # ä½¿ç”¨ç‰¹å¾æ–‡ä»¶ä¸­çš„æ ‡ç­¾åˆ—ï¼ˆç¬¬äºŒå¤©æ¶¨è·Œå¹…ï¼Œ0.1%é˜ˆå€¼ï¼‰
        y = training_data['label'].values
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
        X = X[valid_mask]
        y = y[valid_mask]
        
        print(f"ğŸ” æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(X)} æ¡")
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X.shape}")
        print(f"ğŸ¯ æ ‡ç­¾åˆ†å¸ƒ: ä¸Šæ¶¨ {np.sum(y)} ({np.sum(y)/len(y):.1%}), ä¸‹è·Œ {np.sum(~y)} ({np.sum(~y)/len(y):.1%})")
        
        # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä¿æŒæ—¶é—´é¡ºåºï¼‰
        split_idx = int(len(X) * (1 - test_ratio))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        print(f"ğŸ“š è®­ç»ƒé›†: {len(X_train)} æ¡ ({len(X_train)/len(X):.1%})")
        print(f"ğŸ§ª æµ‹è¯•é›†: {len(X_test)} æ¡ ({len(X_test)/len(X):.1%})")
        
        if return_split:
            return X_train, X_test, y_train, y_test, feature_cols
        else:
            # è¿”å›åˆå¹¶åçš„æ•°æ®ï¼Œç”¨äºæµ‹è¯•
            X_combined = np.vstack([X_train, X_test])
            y_combined = np.concatenate([y_train, y_test])
            dates_combined = np.concatenate([feat.index[:len(X_train)], feat.index[len(X_train):len(X_train)+len(X_test)]])
            return X_combined, y_combined, feature_cols, dates_combined
    
    def get_available_algorithms(self):
        """è·å–å¯ç”¨çš„ç®—æ³•åˆ—è¡¨"""
        algorithms = {
            'random_forest': {
                'name': 'éšæœºæ£®æ—',
                'description': 'é›†æˆå­¦ä¹ ï¼ŒæŠ—è¿‡æ‹Ÿåˆï¼Œé€‚åˆå¤æ‚æ•°æ®',
                'available': True
            },
            'xgboost': {
                'name': 'XGBoost',
                'description': 'æ¢¯åº¦æå‡ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œé€‚åˆç»“æ„åŒ–æ•°æ®',
                'available': XGBOOST_AVAILABLE
            }
        }
        return algorithms
    
    def create_model(self, algorithm='random_forest', random_seed=42):
        """æ ¹æ®ç®—æ³•åç§°åˆ›å»ºæ¨¡å‹
        
        Args:
            algorithm: ç®—æ³•åç§° ('random_forest', 'xgboost')
            random_seed: éšæœºç§å­
            
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        if algorithm == 'random_forest':
            print("ğŸŒ² ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹")
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=random_seed,
                n_jobs=-1
            )
        
        elif algorithm == 'xgboost' and XGBOOST_AVAILABLE:
            print("ğŸš€ ä½¿ç”¨XGBoostæ¨¡å‹")
            return xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_seed,
                n_jobs=-1
            )
        
        else:
            if algorithm == 'xgboost' and not XGBOOST_AVAILABLE:
                print("âš ï¸  XGBoostä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°éšæœºæ£®æ—")
                algorithm = 'random_forest'
            else:
                print(f"âš ï¸  æœªçŸ¥ç®—æ³• '{algorithm}'ï¼Œä½¿ç”¨éšæœºæ£®æ—")
                algorithm = 'random_forest'
            
            return self.create_model('random_forest', random_seed)
    
    def train_model(self, stock_code, algorithm='random_forest', cv_folds=5, start_date=None, end_date=None, test_ratio=None, random_seed=42):
        """è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„ç®—æ³•å’Œæ—¶é—´åˆ’åˆ†"""
        print(f"ğŸ¤– å¼€å§‹è®­ç»ƒæ¨¡å‹: {stock_code}")
        print(f"ğŸ§  ä½¿ç”¨ç®—æ³•: {algorithm}")
        print("=" * 60)
        
        try:
            # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
            X_train, X_test, y_train, y_test, feature_cols = self.prepare_training_data(
                stock_code, start_date, end_date, test_ratio
            )
            
            # 2. æ•°æ®æ ‡å‡†åŒ–
            print("ğŸ“ æ•°æ®æ ‡å‡†åŒ–...")
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # 3. æ¨¡å‹è®­ç»ƒ
            print(f"ğŸ¯ è®­ç»ƒ{self.get_available_algorithms()[algorithm]['name']}æ¨¡å‹...")
            
            # åˆ›å»ºæŒ‡å®šç®—æ³•çš„æ¨¡å‹
            model = self.create_model(algorithm, random_seed)
            
            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv, scoring='accuracy')
            
            print(f"ğŸ“Š äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
            
            # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
            model.fit(X_train_scaled, y_train)
            
            # 4. æ¨¡å‹è¯„ä¼°
            print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
            
            # è®­ç»ƒé›†è¯„ä¼°
            y_train_pred = model.predict(X_train_scaled)
            train_accuracy = accuracy_score(y_train, y_train_pred)
            train_f1 = f1_score(y_train, y_train_pred)
            
            # æµ‹è¯•é›†è¯„ä¼°
            y_test_pred = model.predict(X_test_scaled)
            test_accuracy = accuracy_score(y_test, y_test_pred)
            test_f1 = f1_score(y_test, y_test_pred)
            
            print(f"ğŸ“š è®­ç»ƒé›† - å‡†ç¡®ç‡: {train_accuracy:.3f}, F1åˆ†æ•°: {train_f1:.3f}")
            print(f"ğŸ§ª æµ‹è¯•é›† - å‡†ç¡®ç‡: {test_accuracy:.3f}, F1åˆ†æ•°: {test_f1:.3f}")
            
            # 5. ä¿å­˜æ¨¡å‹å’Œç»“æœ
            self._save_model_and_results(stock_code, model, scaler, feature_cols, {
                'training_date': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'algorithm': algorithm,
                'cv_scores': {
                    'mean': float(cv_scores.mean()),
                    'std': float(cv_scores.std()),
                    'scores': cv_scores.tolist()
                },
                'training_metrics': {
                    'train_accuracy': float(train_accuracy),
                    'train_f1': float(train_f1),
                    'test_accuracy': float(test_accuracy),
                    'test_f1': float(test_f1)
                },
                'data_info': {
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'feature_count': len(feature_cols),
                    'training_start': start_date if start_date else self.default_time_config['training_start'],
                    'training_end': end_date if end_date else self.default_time_config['training_end']
                }
            })
            
            # 6. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            self._generate_training_report(stock_code, y_test, y_test_pred, feature_cols)
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: {stock_code}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def get_validation_data_info(self, stock_code):
        """è·å–éªŒè¯æ•°æ®ä¿¡æ¯ï¼ˆ2025å¹´æ•°æ®ï¼‰"""
        print(f"ğŸ” è·å–éªŒè¯æ•°æ®ä¿¡æ¯: {stock_code}")
        
        feat = self.load_features(stock_code)
        
        validation_start = pd.to_datetime(self.default_time_config['validation_start'])
        validation_end = pd.to_datetime(self.default_time_config['validation_end'])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æ•°æ®
        available_start = feat.index.min()
        available_end = feat.index.max()
        
        if validation_end < available_start or validation_start > available_end:
            print(f"âš ï¸  éªŒè¯æ—¶é—´èŒƒå›´ {validation_start.strftime('%Y-%m-%d')} åˆ° {validation_end.strftime('%Y-%m-%d')} ä¸å¯ç”¨æ•°æ®ä¸é‡å ")
            return None
        
        # è¿‡æ»¤éªŒè¯æ•°æ®
        validation_data = feat[(feat.index >= validation_start) & (feat.index <= validation_end)].copy()
        
        if len(validation_data) == 0:
            print(f"âš ï¸  åœ¨éªŒè¯æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
            return None
        
        info = {
            'validation_start': validation_data.index.min().strftime('%Y-%m-%d'),
            'validation_end': validation_data.index.max().strftime('%Y-%m-%d'),
            'sample_count': len(validation_data),
            'available': True
        }
        
        print(f"ğŸ“Š éªŒè¯æ•°æ®: {info['sample_count']} æ¡è®°å½•")
        print(f"ğŸ“… éªŒè¯æ—¶é—´èŒƒå›´: {info['validation_start']} åˆ° {info['validation_end']}")
        
        return info
    
    def _save_model_and_results(self, stock_code, model, scaler, feature_cols, model_info):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯"""
        stock_model_dir = os.path.join(self.models_dir, stock_code)
        os.makedirs(stock_model_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(stock_model_dir, f"{stock_code}_model_{timestamp}.pkl")
        joblib.dump(model, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {os.path.basename(model_path)}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = os.path.join(stock_model_dir, f"{stock_code}_scaler_{timestamp}.pkl")
        joblib.dump(scaler, scaler_path)
        print(f"ğŸ’¾ æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {os.path.basename(scaler_path)}")
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info['feature_names'] = feature_cols
        info_path = os.path.join(stock_model_dir, f"{stock_code}_info_{timestamp}.json")
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2, default=str)
        print(f"ğŸ’¾ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {os.path.basename(info_path)}")
    
    def _generate_training_report(self, stock_code, y_true, y_pred, feature_cols):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(self.results_dir, 'training_reports', f"{stock_code}_training_report_{timestamp}.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"æ¨¡å‹è®­ç»ƒæŠ¥å‘Š - {stock_code}\n")
            f.write("=" * 50 + "\n")
            f.write(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}\n")
            f.write(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_true)}\n")
            f.write(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy_score(y_true, y_pred):.3f}\n")
            f.write(f"æµ‹è¯•é›†F1åˆ†æ•°: {f1_score(y_true, y_pred):.3f}\n")
            f.write("\nåˆ†ç±»æŠ¥å‘Š:\n")
            f.write(classification_report(y_true, y_pred))
            f.write("\næ··æ·†çŸ©é˜µ:\n")
            f.write(str(confusion_matrix(y_true, y_pred)))
        
        print(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {os.path.basename(report_path)}")
    
    def batch_train_models(self, stock_list=None, algorithm='random_forest', cv_folds=5, start_date=None, end_date=None, random_seed=42):
        """æ‰¹é‡è®­ç»ƒå¤šä¸ªè‚¡ç¥¨çš„æ¨¡å‹
        
        Args:
            stock_list: è‚¡ç¥¨åˆ—è¡¨
            algorithm: ç®—æ³•é€‰æ‹© ('logistic', 'random_forest', 'xgboost')
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            random_seed: éšæœºç§å­
        """
        if stock_list is None:
            # ä»ç‰¹å¾ç›®å½•è·å–è‚¡ç¥¨åˆ—è¡¨
            if os.path.exists(self.features_dir):
                stock_list = [d for d in os.listdir(self.features_dir) if os.path.isdir(os.path.join(self.features_dir, d))]
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾ç›®å½•")
                return {}
        
        if not stock_list:
            print("âŒ æ²¡æœ‰è‚¡ç¥¨å¯è®­ç»ƒ")
            return {}
        
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
        training_start = start_date if start_date else self.default_time_config['training_start']
        training_end = end_date if end_date else self.default_time_config['training_end']
        
        print(f"ğŸš€ æ‰¹é‡è®­ç»ƒæ¨¡å‹: {len(stock_list)} åªè‚¡ç¥¨")
        print("=" * 60)
        print(f"ğŸ§  ä½¿ç”¨ç®—æ³•: {algorithm}")
        print(f"ğŸ“… è®­ç»ƒæ—¶é—´: {training_start} åˆ° {training_end}")
        print(f"ğŸ“… éªŒè¯æ—¶é—´: {self.default_time_config['validation_start']} åˆ° {self.default_time_config['validation_end']}")
        print(f"ğŸ² éšæœºç§å­: {random_seed}")
        print("=" * 60)
        
        results = {}
        
        for i, stock_code in enumerate(stock_list, 1):
            print(f"\nğŸ¤– [{i}/{len(stock_list)}] è®­ç»ƒè‚¡ç¥¨: {stock_code}")
            print("-" * 40)
            
            try:
                # æ£€æŸ¥éªŒè¯æ•°æ®å¯ç”¨æ€§
                validation_info = self.get_validation_data_info(stock_code)
                if not validation_info or not validation_info['available']:
                    print(f"âš ï¸  {stock_code}: éªŒè¯æ•°æ®ä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒ")
                    results[stock_code] = {
                        'success': False,
                        'error': 'éªŒè¯æ•°æ®ä¸å¯ç”¨'
                    }
                    continue
                
                success = self.train_model(stock_code, algorithm, cv_folds, training_start, training_end, random_seed=random_seed)
                
                if success:
                    results[stock_code] = {
                        'success': True,
                        'validation_info': validation_info
                    }
                else:
                    results[stock_code] = {
                        'success': False,
                        'error': 'è®­ç»ƒå¤±è´¥'
                    }
                    
            except Exception as e:
                print(f"âŒ {stock_code}: è®­ç»ƒå¼‚å¸¸ - {e}")
                results[stock_code] = {
                    'success': False,
                    'error': str(e)
                }
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ¨¡å‹è®­ç»ƒä¸ä¿å­˜æ¨¡å—")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨
    # å‡è®¾ç‰¹å¾ç›®å½•ã€æ¨¡å‹ç›®å½•å’Œç»“æœç›®å½•å·²ç»å­˜åœ¨
    features_dir = "features"
    models_dir = "models"
    results_dir = "results"
    
    mt = ModelTrainer(features_dir, models_dir, results_dir)
    
    print("ğŸ”§ ç³»ç»ŸçŠ¶æ€:")
    print(f"   ç‰¹å¾ç›®å½•: {os.path.abspath(mt.features_dir)}")
    print(f"   æ¨¡å‹ç›®å½•: {os.path.abspath(mt.models_dir)}")
    print(f"   ç»“æœç›®å½•: {os.path.abspath(mt.results_dir)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾æ•°æ®
    if not os.path.exists(mt.features_dir):
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾ç›®å½•!")
        print("   è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹æ¨¡å—æ„å»ºç‰¹å¾")
        return
    
    # è·å–å¯ç”¨çš„è‚¡ç¥¨
    stock_dirs = [d for d in os.listdir(mt.features_dir) if os.path.isdir(os.path.join(mt.features_dir, d))]
    
    if not stock_dirs:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ•°æ®!")
        print("   è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹æ¨¡å—æ„å»ºç‰¹å¾")
        return
    
    print(f"\nğŸ“Š å‘ç° {len(stock_dirs)} åªè‚¡ç¥¨çš„ç‰¹å¾:")
    for stock_dir in stock_dirs:
        print(f"   â€¢ {stock_dir}")
    
    # å¼€å§‹è®­ç»ƒæ¨¡å‹
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    results = mt.batch_train_models(stock_list=stock_dirs)
    
    # æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦
    print(f"\nï¿½ï¿½ æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒæŸ¥çœ‹æ‘˜è¦:")
    # mt.get_model_summary() # This method is removed from the new_code, so we'll skip this for now.

if __name__ == "__main__":
    main()
